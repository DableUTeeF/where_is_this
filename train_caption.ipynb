{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 14:20:15.045632: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-10 14:20:15.298346: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "from transformers import AutoModel, CLIPProcessor\n",
    "from models import WhereIsFeatures\n",
    "from dataset import FolderData, COCOCaptionData\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "from torch import nn\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(text_embeds, image_embeds, labels):\n",
    "    logits_per_image = torch.matmul(text_embeds, image_embeds.t()).t()\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    return (probs.argmax(1) == labels).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "n_epochs = 5\n",
    "warmup = 4\n",
    "num_workers = 4\n",
    "batch_size = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = '/home/palm/data/coco/annotations/annotations/captions_train2017.json'\n",
    "test_src = '/home/palm/data/dogs-vs-cats/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = COCOCaptionData(train_src)\n",
    "val_dataset = FolderData(test_src, size=224, mul=1)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of a cat', 'a photo of a dog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = []\n",
    "for folder in sorted(os.listdir(test_src)):\n",
    "    test_texts.append(f'a photo of a {folder}')\n",
    "test_inputs = processor(text=test_texts, return_tensors=\"pt\", padding=True)\n",
    "test_texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "clip = AutoModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "for param in clip.parameters():\n",
    "    param.requires_grad = False\n",
    "vision_model = clip.vision_model\n",
    "visual_projection = clip.visual_projection\n",
    "text_projection = clip.text_projection\n",
    "test_prompts = clip.text_model(**test_inputs.to(device))\n",
    "test_prompts = sigmoid(text_projection(test_prompts[1]))\n",
    "model = WhereIsFeatures()\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 71s 24ms/step - loss: 5.9012e-04\n",
      "1250/1250 [==============================] - 45s 35ms/step - loss: 0.0022 - std_acc: 0.9911 - recon_acc: 0.9905\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 76s 25ms/step - loss: 1.1359e-04\n",
      "1250/1250 [==============================] - 44s 35ms/step - loss: 0.0017 - std_acc: 0.9915 - recon_acc: 0.9914\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                                t_initial=5,\n",
    "                                t_mul=1,\n",
    "                                lr_min=5e-5,\n",
    "                                decay_rate=0.1,\n",
    "                                cycle_limit=1,\n",
    "                                t_in_epochs=False,\n",
    "                                noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(2):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to(device))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "\n",
    "        x = model.encode(features)\n",
    "        recon = model.decode(x)\n",
    "        loss = mse(recon, features)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            x = model.encode(features)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(recon, features)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: buffer nowhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 98s 33ms/step - loss: 7.8951\n",
      "1250/1250 [==============================] - 48s 39ms/step - loss: 4.8610 - std_acc: 0.9918 - recon_acc: 0.5069 - buffer_acc: 0.9182\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 97s 32ms/step - loss: 7.8745\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 4.8491 - std_acc: 0.9911 - recon_acc: 0.6448 - buffer_acc: 0.9873\n",
      "Epoch: 3\n",
      "3000/3000 [==============================] - 99s 33ms/step - loss: 7.8696\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 4.8481 - std_acc: 0.9915 - recon_acc: 0.7421 - buffer_acc: 0.9909\n",
      "Epoch: 4\n",
      "3000/3000 [==============================] - 101s 33ms/step - loss: 7.8652\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 4.8451 - std_acc: 0.9914 - recon_acc: 0.8058 - buffer_acc: 0.9915\n",
      "Epoch: 5\n",
      "3000/3000 [==============================] - 97s 32ms/step - loss: 7.8696\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 4.8373 - std_acc: 0.9918 - recon_acc: 0.7466 - buffer_acc: 0.9908\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                             warmup_t=1,\n",
    "                             warmup_lr_init=1e-5,\n",
    "                             t_initial=n_epochs,\n",
    "                             t_mul=1,\n",
    "                             lr_min=5e-5,\n",
    "                             decay_rate=0.1,\n",
    "                             cycle_limit=1,\n",
    "                             t_in_epochs=False,\n",
    "                             noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to('cuda'))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "            x = model.encode(features)\n",
    "\n",
    "        x, ecd, gt = model.where(x, False)\n",
    "        loss = mse(x, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(test_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, False)\n",
    "            x = model.encode(features)\n",
    "            x, ecd, gt = model.where(x, False)\n",
    "            buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(x, gt)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: buffer where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 100s 33ms/step - loss: 7.8762\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 4.8498 - std_acc: 0.9922 - recon_acc: 0.6967 - buffer_acc: 0.9621\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 99s 33ms/step - loss: 7.8741\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 4.8474 - std_acc: 0.9913 - recon_acc: 0.6610 - buffer_acc: 0.9623\n",
      "Epoch: 3\n",
      "3000/3000 [==============================] - 102s 34ms/step - loss: 7.8699\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 4.8463 - std_acc: 0.9916 - recon_acc: 0.6812 - buffer_acc: 0.9624\n",
      "Epoch: 4\n",
      "3000/3000 [==============================] - 98s 33ms/step - loss: 7.8715\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 4.8432 - std_acc: 0.9909 - recon_acc: 0.6970 - buffer_acc: 0.9633\n",
      "Epoch: 5\n",
      "3000/3000 [==============================] - 100s 33ms/step - loss: 7.8686\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 4.8452 - std_acc: 0.9917 - recon_acc: 0.7142 - buffer_acc: 0.9636\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                             warmup_t=1,\n",
    "                             warmup_lr_init=1e-5,\n",
    "                             t_initial=n_epochs,\n",
    "                             t_mul=1,\n",
    "                             lr_min=5e-5,\n",
    "                             decay_rate=0.1,\n",
    "                             cycle_limit=1,\n",
    "                             t_in_epochs=False,\n",
    "                             noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to('cuda'))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "            x = model.encode(features)\n",
    "\n",
    "        x, ecd, gt = model.where(x, True)\n",
    "        loss = mse(x, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(test_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, True)\n",
    "            x = model.encode(features)\n",
    "            x, ecd, gt = model.where(x, True)\n",
    "            buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(x, gt)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tanh eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 50s 40ms/step - loss: 4.8460 - std_acc: 0.9915 - recon_acc: 0.7151 - buffer_acc: 0.9883\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "with torch.no_grad():\n",
    "    for idx, (image, _, cls) in enumerate(test_loader):\n",
    "        image = image.to(device)\n",
    "        cls = cls.to(device)\n",
    "        features = vision_model(image)['pooler_output']\n",
    "        features = visual_projection(features)\n",
    "        features = sigmoid(features)\n",
    "        std_acc = accuracy(test_prompts, features, cls)\n",
    "        prompts_ecd = model.encode(test_prompts)\n",
    "        _, prompts_ecd, _ = model.where(prompts_ecd, True)\n",
    "        x = model.encode(features)\n",
    "        x, ecd, gt = model.where(x, True)\n",
    "        buffer_acc = accuracy(prompts_ecd[:, 0]*2-1, ecd[:, 0]*2-1, cls)\n",
    "        recon = model.decode(x)\n",
    "        recon_acc = accuracy(test_prompts, recon, cls)\n",
    "        loss = mse(x, gt)\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                    ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                    ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47aa4ac3713e7d9a8d444fbbb3116eb5ae369e2c784d7b1a13c1cbac73ef9c1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
