{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 12:26:59.613045: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-06 12:26:59.864539: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "from transformers import AutoModel, CLIPProcessor\n",
    "from models import WhereIsFeatures\n",
    "from dataset import FolderData, COCOCaptionData\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "from torch import nn\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(text_embeds, image_embeds, labels):\n",
    "    logits_per_image = torch.matmul(text_embeds, image_embeds.t()).t()\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    return (probs.argmax(1) == labels).float().mean()\n",
    "\n",
    "def clip_tanh_accuracy(text_embeds, image_embeds, labels):\n",
    "    logits_per_image = torch.matmul(text_embeds*2-1, (image_embeds*2-1).t()).t()\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    return (probs.argmax(1) == labels).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      3\u001b[0m warmup \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m      4\u001b[0m num_workers \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      3\u001b[0m warmup \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m      4\u001b[0m num_workers \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/clipme/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/clipme/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "n_epochs = 5\n",
    "warmup = 4\n",
    "num_workers = 4\n",
    "batch_size = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = '/home/palm/data/coco/annotations/annotations/captions_train2017.json'\n",
    "test_src = '/home/palm/data/dogs-vs-cats/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = COCOCaptionData(train_src)\n",
    "val_dataset = FolderData(test_src, size=224, mul=1)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of a cat', 'a photo of a dog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = []\n",
    "for folder in sorted(os.listdir(test_src)):\n",
    "    test_texts.append(f'a photo of a {folder}')\n",
    "test_inputs = processor(text=test_texts, return_tensors=\"pt\", padding=True)\n",
    "test_texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "clip = AutoModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "for param in clip.parameters():\n",
    "    param.requires_grad = False\n",
    "vision_model = clip.vision_model\n",
    "visual_projection = clip.visual_projection\n",
    "text_projection = clip.text_projection\n",
    "test_prompts = clip.text_model(**test_inputs.to(device))\n",
    "test_prompts = sigmoid(text_projection(test_prompts[1]))\n",
    "model = WhereIsFeatures(1500)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 68s 23ms/step - loss: 5.9603e-04\n",
      "1250/1250 [==============================] - 45s 35ms/step - loss: 0.0021 - std_acc: 0.9909 - recon_acc: 0.9911\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 71s 24ms/step - loss: 1.1466e-04\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 0.0017 - std_acc: 0.9914 - recon_acc: 0.9908\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                                t_initial=5,\n",
    "                                t_mul=1,\n",
    "                                lr_min=5e-5,\n",
    "                                decay_rate=0.1,\n",
    "                                cycle_limit=1,\n",
    "                                t_in_epochs=False,\n",
    "                                noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(2):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to(device))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "\n",
    "        x = model.encode(features)\n",
    "        recon = model.decode(x)\n",
    "        loss = mse(recon, features)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            x = model.encode(features)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(recon, features)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: buffer nowhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 92s 31ms/step - loss: 7.9797\n",
      "1250/1250 [==============================] - 48s 38ms/step - loss: 5.2465 - std_acc: 0.9919 - recon_acc: 0.5000 - buffer_acc: 0.9090\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 92s 30ms/step - loss: 7.9539\n",
      "1250/1250 [==============================] - 48s 38ms/step - loss: 5.2350 - std_acc: 0.9914 - recon_acc: 0.5000 - buffer_acc: 0.9600\n",
      "Epoch: 3\n",
      "3000/3000 [==============================] - 92s 31ms/step - loss: 7.9546\n",
      "1250/1250 [==============================] - 48s 38ms/step - loss: 5.2288 - std_acc: 0.9910 - recon_acc: 0.4997 - buffer_acc: 0.9576\n",
      "Epoch: 4\n",
      "3000/3000 [==============================] - 95s 32ms/step - loss: 7.9494\n",
      "1250/1250 [==============================] - 48s 38ms/step - loss: 5.2220 - std_acc: 0.9910 - recon_acc: 0.4999 - buffer_acc: 0.9546\n",
      "Epoch: 5\n",
      "3000/3000 [==============================] - 92s 31ms/step - loss: 7.9470\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 5.2234 - std_acc: 0.9909 - recon_acc: 0.5000 - buffer_acc: 0.9437\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                             warmup_t=1,\n",
    "                             warmup_lr_init=1e-5,\n",
    "                             t_initial=n_epochs,\n",
    "                             t_mul=1,\n",
    "                             lr_min=5e-5,\n",
    "                             decay_rate=0.1,\n",
    "                             cycle_limit=1,\n",
    "                             t_in_epochs=False,\n",
    "                             noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to('cuda'))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "            x = model.encode(features)\n",
    "\n",
    "        x, ecd, gt = model.where(x, False)\n",
    "        loss = mse(x, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(test_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, False)\n",
    "            x = model.encode(features)\n",
    "            x, ecd, gt = model.where(x, False)\n",
    "            buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(x, gt)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: buffer where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 73s 24ms/step - loss: 8.2935\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5272 - std_acc: 0.9919 - sigmoid_acc: 0.9367 - tanh_acc: 0.7302\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 85s 28ms/step - loss: 8.2935\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5271 - std_acc: 0.9912 - sigmoid_acc: 0.9352 - tanh_acc: 0.7351\n",
      "Epoch: 3\n",
      "3000/3000 [==============================] - 85s 28ms/step - loss: 8.2952\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 5.5234 - std_acc: 0.9911 - sigmoid_acc: 0.9362 - tanh_acc: 0.7326\n",
      "Epoch: 4\n",
      "3000/3000 [==============================] - 85s 28ms/step - loss: 8.2914\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5261 - std_acc: 0.9917 - sigmoid_acc: 0.9373 - tanh_acc: 0.7336\n",
      "Epoch: 5\n",
      "3000/3000 [==============================] - 85s 28ms/step - loss: 8.2957\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 5.5289 - std_acc: 0.9912 - sigmoid_acc: 0.9372 - tanh_acc: 0.7362\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                             warmup_t=1,\n",
    "                             warmup_lr_init=1e-5,\n",
    "                             t_initial=n_epochs,\n",
    "                             t_mul=1,\n",
    "                             lr_min=5e-5,\n",
    "                             decay_rate=0.1,\n",
    "                             cycle_limit=1,\n",
    "                             t_in_epochs=False,\n",
    "                             noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to('cuda'))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "            x = model.encode(features)\n",
    "\n",
    "        x, ecd, gt = model.where(x, True)\n",
    "        loss = mse(x, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(test_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, True)\n",
    "            x = model.encode(features)\n",
    "            x, ecd, gt = model.where(x, True)\n",
    "            buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "            buffer_acc_tanh = accuracy(prompts_ecd[:, 0]*2-1, ecd[:, 0]*2-1, cls)\n",
    "            loss = mse(x, gt)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('sigmoid_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                        ('tanh_acc', buffer_acc_tanh.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tanh eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5285 - std_acc: 0.9908 - recon_acc: 0.5001 - buffer_acc: 0.7317\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "with torch.no_grad():\n",
    "    for idx, (image, _, cls) in enumerate(test_loader):\n",
    "        image = image.to(device)\n",
    "        cls = cls.to(device)\n",
    "        features = vision_model(image)['pooler_output']\n",
    "        features = visual_projection(features)\n",
    "        features = sigmoid(features)\n",
    "        std_acc = accuracy(test_prompts, features, cls)\n",
    "        prompts_ecd = model.encode(test_prompts)\n",
    "        _, prompts_ecd, _ = model.where(prompts_ecd, True)\n",
    "        x = model.encode(features)\n",
    "        x, ecd, gt = model.where(x, True)\n",
    "        buffer_acc = accuracy(prompts_ecd[:, 0]*2-1, ecd[:, 0]*2-1, cls)\n",
    "        recon = model.decode(x)\n",
    "        recon_acc = accuracy(test_prompts, recon, cls)\n",
    "        loss = mse(x, gt)\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                    ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                    ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47aa4ac3713e7d9a8d444fbbb3116eb5ae369e2c784d7b1a13c1cbac73ef9c1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
