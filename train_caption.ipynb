{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 11:25:04.167608: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-13 11:25:04.468762: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "from transformers import AutoModel, CLIPProcessor\n",
    "from models import WhereIsFeatures\n",
    "from dataset import FolderData, COCOCaptionData\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "from torch import nn\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(text_embeds, image_embeds, labels):\n",
    "    logits_per_image = torch.matmul(text_embeds, image_embeds.t()).t()\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    return (probs.argmax(1) == labels).float().mean()\n",
    "\n",
    "def clip_tanh_accuracy(text_embeds, image_embeds, labels):\n",
    "    logits_per_image = torch.matmul(text_embeds*2-1, (image_embeds*2-1).t()).t()\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    return (probs.argmax(1) == labels).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "n_epochs = 5\n",
    "warmup = 4\n",
    "num_workers = 4\n",
    "batch_size = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = '/home/palm/data/coco/annotations/annotations/captions_train2017.json'\n",
    "test_src = '/home/palm/data/dogs-vs-cats/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = COCOCaptionData(train_src)\n",
    "val_dataset = FolderData(test_src, size=224, mul=1)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of a cat', 'a photo of a dog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = []\n",
    "for folder in sorted(os.listdir(test_src)):\n",
    "    test_texts.append(f'a photo of a {folder}')\n",
    "test_inputs = processor(text=test_texts, return_tensors=\"pt\", padding=True)\n",
    "test_texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "clip = AutoModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "for param in clip.parameters():\n",
    "    param.requires_grad = False\n",
    "vision_model = clip.vision_model\n",
    "visual_projection = clip.visual_projection\n",
    "text_projection = clip.text_projection\n",
    "test_prompts = clip.text_model(**test_inputs.to(device))\n",
    "test_prompts = sigmoid(text_projection(test_prompts[1]))\n",
    "model = WhereIsFeatures(1500)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 59s 19ms/step - loss: 5.7480e-04\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.0022 - std_acc: 0.9916 - recon_acc: 0.9914\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 69s 23ms/step - loss: 1.1385e-04\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.0018 - std_acc: 0.9908 - recon_acc: 0.9906\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                                t_initial=5,\n",
    "                                t_mul=1,\n",
    "                                lr_min=5e-5,\n",
    "                                decay_rate=0.1,\n",
    "                                cycle_limit=1,\n",
    "                                t_in_epochs=False,\n",
    "                                noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(2):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to(device))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "\n",
    "        x = model.encode(features)\n",
    "        recon = model.decode(x)\n",
    "        loss = mse(recon, features)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            x = model.encode(features)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(recon, features)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: buffer nowhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 82s 27ms/step - loss: 8.3169\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5325 - std_acc: 0.9918 - recon_acc: 0.5000 - buffer_acc: 0.6292\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 81s 27ms/step - loss: 8.2945\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5203 - std_acc: 0.9909 - recon_acc: 0.5000 - buffer_acc: 0.9857\n",
      "Epoch: 3\n",
      "3000/3000 [==============================] - 81s 27ms/step - loss: 8.2895\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5176 - std_acc: 0.9912 - recon_acc: 0.5000 - buffer_acc: 0.9857\n",
      "Epoch: 4\n",
      "3000/3000 [==============================] - 82s 27ms/step - loss: 8.2911\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5168 - std_acc: 0.9913 - recon_acc: 0.5000 - buffer_acc: 0.9880\n",
      "Epoch: 5\n",
      "3000/3000 [==============================] - 82s 27ms/step - loss: 8.2899\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5115 - std_acc: 0.9905 - recon_acc: 0.5004 - buffer_acc: 0.9892\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                             warmup_t=1,\n",
    "                             warmup_lr_init=1e-5,\n",
    "                             t_initial=n_epochs,\n",
    "                             t_mul=1,\n",
    "                             lr_min=5e-5,\n",
    "                             decay_rate=0.1,\n",
    "                             cycle_limit=1,\n",
    "                             t_in_epochs=False,\n",
    "                             noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to('cuda'))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "            x = model.encode(features)\n",
    "\n",
    "        x, ecd, gt = model.where(x, False)\n",
    "        loss = mse(x, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(test_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, False)\n",
    "            x = model.encode(features)\n",
    "            x, ecd, gt = model.where(x, False)\n",
    "            buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(x, gt)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: buffer where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 73s 24ms/step - loss: 8.2935\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5272 - std_acc: 0.9919 - sigmoid_acc: 0.9367 - tanh_acc: 0.7302\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 85s 28ms/step - loss: 8.2935\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5271 - std_acc: 0.9912 - sigmoid_acc: 0.9352 - tanh_acc: 0.7351\n",
      "Epoch: 3\n",
      "3000/3000 [==============================] - 85s 28ms/step - loss: 8.2952\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 5.5234 - std_acc: 0.9911 - sigmoid_acc: 0.9362 - tanh_acc: 0.7326\n",
      "Epoch: 4\n",
      "3000/3000 [==============================] - 85s 28ms/step - loss: 8.2914\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5261 - std_acc: 0.9917 - sigmoid_acc: 0.9373 - tanh_acc: 0.7336\n",
      "Epoch: 5\n",
      "3000/3000 [==============================] - 85s 28ms/step - loss: 8.2957\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 5.5289 - std_acc: 0.9912 - sigmoid_acc: 0.9372 - tanh_acc: 0.7362\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                             warmup_t=1,\n",
    "                             warmup_lr_init=1e-5,\n",
    "                             t_initial=n_epochs,\n",
    "                             t_mul=1,\n",
    "                             lr_min=5e-5,\n",
    "                             decay_rate=0.1,\n",
    "                             cycle_limit=1,\n",
    "                             t_in_epochs=False,\n",
    "                             noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to('cuda'))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "            x = model.encode(features)\n",
    "\n",
    "        x, ecd, gt = model.where(x, True)\n",
    "        loss = mse(x, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(test_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, True)\n",
    "            x = model.encode(features)\n",
    "            x, ecd, gt = model.where(x, True)\n",
    "            buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "            buffer_acc_tanh = accuracy(prompts_ecd[:, 0]*2-1, ecd[:, 0]*2-1, cls)\n",
    "            loss = mse(x, gt)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('sigmoid_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                        ('tanh_acc', buffer_acc_tanh.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tanh eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5285 - std_acc: 0.9908 - recon_acc: 0.5001 - buffer_acc: 0.7317\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "with torch.no_grad():\n",
    "    for idx, (image, _, cls) in enumerate(test_loader):\n",
    "        image = image.to(device)\n",
    "        cls = cls.to(device)\n",
    "        features = vision_model(image)['pooler_output']\n",
    "        features = visual_projection(features)\n",
    "        features = sigmoid(features)\n",
    "        std_acc = accuracy(test_prompts, features, cls)\n",
    "        prompts_ecd = model.encode(test_prompts)\n",
    "        _, prompts_ecd, _ = model.where(prompts_ecd, True)\n",
    "        x = model.encode(features)\n",
    "        x, ecd, gt = model.where(x, True)\n",
    "        buffer_acc = accuracy(prompts_ecd[:, 0]*2-1, ecd[:, 0]*2-1, cls)\n",
    "        recon = model.decode(x)\n",
    "        recon_acc = accuracy(test_prompts, recon, cls)\n",
    "        loss = mse(x, gt)\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                    ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                    ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47aa4ac3713e7d9a8d444fbbb3116eb5ae369e2c784d7b1a13c1cbac73ef9c1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
