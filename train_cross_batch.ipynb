{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 20:33:46.897895: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-24 20:33:47.185779: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "from transformers import AutoModel, CLIPProcessor\n",
    "from models import WhereIsFeatures\n",
    "from dataset import FolderData, COCOCaptionData\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "from torch import nn\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(text_embeds, image_embeds, labels):\n",
    "    logits_per_image = torch.matmul(text_embeds, image_embeds.t()).t()\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    return (probs.argmax(1) == labels).float().mean()\n",
    "\n",
    "def clip_tanh_accuracy(text_embeds, image_embeds, labels):\n",
    "    logits_per_image = torch.matmul(text_embeds*2-1, (image_embeds*2-1).t()).t()\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    return (probs.argmax(1) == labels).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "n_epochs = 5\n",
    "warmup = 4\n",
    "num_workers = 4\n",
    "batch_size = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = '/home/palm/data/coco/annotations/annotations/captions_train2017.json'\n",
    "test_src = '/home/palm/data/dogs-vs-cats/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = COCOCaptionData(train_src)\n",
    "val_dataset = FolderData(test_src, size=224, mul=1)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of a cat', 'a photo of a dog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = []\n",
    "for folder in sorted(os.listdir(test_src)):\n",
    "    test_texts.append(f'a photo of a {folder}')\n",
    "test_inputs = processor(text=test_texts, return_tensors=\"pt\", padding=True)\n",
    "test_texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossMSE(nn.Module):\n",
    "    def forward(self, predict, target):\n",
    "        predict = predict[:, 0]\n",
    "        target = target[:, 0]\n",
    "        p = torch.matmul(predict, predict.t()).t().softmax(dim=1)\n",
    "        q = torch.matmul(target, target.t()).t().softmax(dim=1)\n",
    "        return -(p * torch.log(q + 1e-8)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "crossbatch = CrossMSE()\n",
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = AutoModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "for param in clip.parameters():\n",
    "    param.requires_grad = False\n",
    "vision_model = clip.vision_model\n",
    "visual_projection = clip.visual_projection\n",
    "text_projection = clip.text_projection\n",
    "test_prompts = clip.text_model(**test_inputs.to(device))\n",
    "test_prompts = sigmoid(text_projection(test_prompts[1]))\n",
    "model = WhereIsFeatures(1500)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 72s 24ms/step - loss: 6.0246e-04\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.0021 - std_acc: 0.9907 - recon_acc: 0.9904\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 73s 24ms/step - loss: 1.1550e-04\n",
      "1250/1250 [==============================] - 43s 35ms/step - loss: 0.0016 - std_acc: 0.9913 - recon_acc: 0.9913\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                                t_initial=5,\n",
    "                                t_mul=1,\n",
    "                                lr_min=5e-5,\n",
    "                                decay_rate=0.1,\n",
    "                                cycle_limit=1,\n",
    "                                t_in_epochs=False,\n",
    "                                noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(2):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to(device))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "\n",
    "        x = model.encode(features)\n",
    "        recon = model.decode(x)\n",
    "        loss = mse(recon, features)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            x = model.encode(features)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(recon, features)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: buffer nowhere mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 90s 30ms/step - loss: 8.0695\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.4906 - std_acc: 0.9911 - recon_acc: 0.5663 - buffer_acc: 0.9785\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 90s 30ms/step - loss: 8.0448\n",
      "1250/1250 [==============================] - 47s 37ms/step - loss: 5.4759 - std_acc: 0.9916 - recon_acc: 0.7183 - buffer_acc: 0.9891\n",
      "Epoch: 3\n",
      "3000/3000 [==============================] - 89s 30ms/step - loss: 8.0379\n",
      "1250/1250 [==============================] - 47s 37ms/step - loss: 5.4655 - std_acc: 0.9913 - recon_acc: 0.6589 - buffer_acc: 0.9863\n",
      "Epoch: 4\n",
      "3000/3000 [==============================] - 91s 30ms/step - loss: 8.0403\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.4635 - std_acc: 0.9908 - recon_acc: 0.6685 - buffer_acc: 0.9886\n",
      "Epoch: 5\n",
      "3000/3000 [==============================] - 87s 29ms/step - loss: 8.0409\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.4581 - std_acc: 0.9907 - recon_acc: 0.5412 - buffer_acc: 0.9897\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                             warmup_t=1,\n",
    "                             warmup_lr_init=1e-5,\n",
    "                             t_initial=n_epochs,\n",
    "                             t_mul=1,\n",
    "                             lr_min=5e-5,\n",
    "                             decay_rate=0.1,\n",
    "                             cycle_limit=1,\n",
    "                             t_in_epochs=False,\n",
    "                             noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to('cuda'))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "            x = model.encode(features)\n",
    "\n",
    "        x, ecd, gt = model.where(x, False)\n",
    "        loss = mse(x, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy())\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(test_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, False)\n",
    "            x = model.encode(features)\n",
    "            x, ecd, gt = model.where(x, False)\n",
    "            buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(x, gt)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: buffer nowhere crossbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = x[:, 0]\n",
    "target = ecd[:, 0]\n",
    "p = torch.matmul(predict, predict.t()).t().softmax(dim=1)\n",
    "q = torch.matmul(target, target.t()).t().softmax(dim=1)\n",
    "l = -(p * q.log()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.4948, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 87s 29ms/step - loss: 8.1379\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.4768 - std_acc: 0.9909 - recon_acc: 0.5023 - buffer_acc: 0.9883\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 87s 29ms/step - loss: 8.1056\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5082 - std_acc: 0.9914 - recon_acc: 0.5000 - buffer_acc: 0.9893\n",
      "Epoch: 3\n",
      "3000/3000 [==============================] - 87s 29ms/step - loss: 8.0890\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.4968 - std_acc: 0.9920 - recon_acc: 0.5011 - buffer_acc: 0.9892\n",
      "Epoch: 4\n",
      "3000/3000 [==============================] - 87s 29ms/step - loss: 8.1093\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.4907 - std_acc: 0.9917 - recon_acc: 0.5290 - buffer_acc: 0.9862\n",
      "Epoch: 5\n",
      "3000/3000 [==============================] - 87s 29ms/step - loss: 8.0755\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.5680 - std_acc: 0.9919 - recon_acc: 0.5110 - buffer_acc: 0.9860\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                             warmup_t=1,\n",
    "                             warmup_lr_init=1e-5,\n",
    "                             t_initial=n_epochs,\n",
    "                             t_mul=1,\n",
    "                             lr_min=5e-5,\n",
    "                             decay_rate=0.1,\n",
    "                             cycle_limit=1,\n",
    "                             t_in_epochs=False,\n",
    "                             noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to('cuda'))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "            x = model.encode(features)\n",
    "\n",
    "        x, ecd, gt = model.where(x, False)\n",
    "        loss = crossbatch(x, ecd) + mse(x, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy())\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(test_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, False)\n",
    "            x = model.encode(features)\n",
    "            x, ecd, gt = model.where(x, False)\n",
    "            buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(x, gt)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: buffer where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "3000/3000 [==============================] - 75s 25ms/step - loss: 8.0747\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 5.5014 - std_acc: 0.9909 - sigmoid_acc: 0.9762 - tanh_acc: 0.9803\n",
      "Epoch: 2\n",
      "3000/3000 [==============================] - 88s 29ms/step - loss: 8.0743\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 5.4976 - std_acc: 0.9910 - sigmoid_acc: 0.9748 - tanh_acc: 0.9791\n",
      "Epoch: 3\n",
      "3000/3000 [==============================] - 88s 29ms/step - loss: 8.0705\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 5.4981 - std_acc: 0.9911 - sigmoid_acc: 0.9746 - tanh_acc: 0.9798\n",
      "Epoch: 4\n",
      "3000/3000 [==============================] - 87s 29ms/step - loss: 8.0702\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 5.5013 - std_acc: 0.9907 - sigmoid_acc: 0.9752 - tanh_acc: 0.9791\n",
      "Epoch: 5\n",
      "3000/3000 [==============================] - 87s 29ms/step - loss: 8.0704\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 5.4982 - std_acc: 0.9913 - sigmoid_acc: 0.9744 - tanh_acc: 0.9786\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                             warmup_t=1,\n",
    "                             warmup_lr_init=1e-5,\n",
    "                             t_initial=n_epochs,\n",
    "                             t_mul=1,\n",
    "                             lr_min=5e-5,\n",
    "                             decay_rate=0.1,\n",
    "                             cycle_limit=1,\n",
    "                             t_in_epochs=False,\n",
    "                             noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(3000)\n",
    "    for idx, (captions, cls) in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            captions = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "            features = clip.text_model(**captions.to('cuda'))\n",
    "            features = text_projection(features[1])\n",
    "            features = sigmoid(features)\n",
    "            x = model.encode(features)\n",
    "\n",
    "        x, ecd, gt = model.where(x, True)\n",
    "        loss = crossbatch(x, ecd) + mse(x, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "        if idx > 2998:\n",
    "            break\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(test_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, True)\n",
    "            x = model.encode(features)\n",
    "            x, ecd, gt = model.where(x, True)\n",
    "            buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "            buffer_acc_tanh = accuracy(prompts_ecd[:, 0]*2-1, ecd[:, 0]*2-1, cls)\n",
    "            loss = mse(x, gt)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('sigmoid_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                        ('tanh_acc', buffer_acc_tanh.cpu().detach().numpy()),\n",
    "                        ]\n",
    "            progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tanh eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 48s 38ms/step - loss: 5.5010 - std_acc: 0.9901 - recon_acc: 0.6163 - buffer_acc: 0.9795\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "with torch.no_grad():\n",
    "    for idx, (image, _, cls) in enumerate(test_loader):\n",
    "        image = image.to(device)\n",
    "        cls = cls.to(device)\n",
    "        features = vision_model(image)['pooler_output']\n",
    "        features = visual_projection(features)\n",
    "        features = sigmoid(features)\n",
    "        std_acc = accuracy(test_prompts, features, cls)\n",
    "        prompts_ecd = model.encode(test_prompts)\n",
    "        _, prompts_ecd, _ = model.where(prompts_ecd, True)\n",
    "        x = model.encode(features)\n",
    "        x, ecd, gt = model.where(x, True)\n",
    "        buffer_acc = accuracy(prompts_ecd[:, 0]*2-1, ecd[:, 0]*2-1, cls)\n",
    "        recon = model.decode(x)\n",
    "        recon_acc = accuracy(test_prompts, recon, cls)\n",
    "        loss = mse(x, gt)\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                    ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                    ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embed = torch.rand((50, 4))\n",
    "image_embed = torch.rand((50, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (torch.rand(50) * 50 + 50).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([52, 51, 87, 84, 78, 71, 99, 98, 60, 52, 79, 70, 87, 78, 83, 52, 63, 93,\n",
       "        93, 51, 65, 63, 82, 65, 77, 98, 54, 52, 99, 94, 90, 74, 97, 71, 77, 58,\n",
       "        99, 51, 67, 59, 76, 50, 80, 68, 86, 58, 77, 86, 90, 92],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21, 21, 21, 45, 21, 21, 21,  9, 19, 21, 21, 19, 21, 21, 21, 19, 19, 21,\n",
       "        21, 21, 45, 21, 21, 21, 21,  9, 21, 21, 21, 45, 21,  9, 21, 21,  9,  9,\n",
       "         9, 21, 19, 21, 21, 21, 45, 21, 45, 45,  9, 45, 45, 19])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = (text_embed @ text_embed.t()).softmax(1)\n",
    "probs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([63, 63, 63, 58, 63, 63, 63, 52, 51, 63, 63, 51, 63, 63, 63, 51, 51, 63,\n",
       "        63, 63, 58, 63, 63, 63, 63, 52, 63, 63, 63, 58, 63, 52, 63, 63, 52, 52,\n",
       "        52, 63, 51, 63, 63, 63, 58, 63, 58, 58, 52, 58, 58, 51],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[probs.argmax(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True, False, False, False, False])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[probs.argmax(1)]==labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47aa4ac3713e7d9a8d444fbbb3116eb5ae369e2c784d7b1a13c1cbac73ef9c1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
