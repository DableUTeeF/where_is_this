{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 11:18:52.829888: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-10 11:18:52.853949: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "from transformers import AutoModel, CLIPProcessor\n",
    "from models import WhereIsFeatures\n",
    "from dataset import FolderData\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "from torch import nn\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(text_embeds, image_embeds, labels):\n",
    "    logits_per_image = torch.matmul(text_embeds, image_embeds.t()).t()\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    return (probs.argmax(1) == labels).float().mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "n_epochs = 5\n",
    "warmup = 4\n",
    "num_workers = 4\n",
    "batch_size = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_src = '/home/palm/data/animals/moved'\n",
    "train_src = '/home/palm/data/dogs-vs-cats/train'\n",
    "\n",
    "train_src = '/home/palm/data/animals/animals/animals'\n",
    "test_src = '/home/palm/data/dogs-vs-cats/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FolderData(train_src, size=224, mul=1)\n",
    "val_dataset = FolderData(test_src, size=224, mul=1)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "test_texts = []\n",
    "for folder in sorted(os.listdir(test_src)):\n",
    "    if 'otter' in folder:\n",
    "        test_texts.append(f'boooooo')\n",
    "        continue\n",
    "    test_texts.append(f'a photo of a {folder}')\n",
    "test_inputs = processor(text=test_texts, return_tensors=\"pt\", padding=True)\n",
    "train_texts = []\n",
    "for folder in sorted(os.listdir(train_src)):\n",
    "    if 'otter' in folder:\n",
    "        train_texts.append(f'boooooo')\n",
    "        continue\n",
    "    train_texts.append(f'a photo of a {folder}')\n",
    "train_inputs = processor(text=train_texts, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of a cat', 'a photo of a dog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of a antelope',\n",
       " 'a photo of a badger',\n",
       " 'a photo of a bat',\n",
       " 'a photo of a bear',\n",
       " 'a photo of a bee',\n",
       " 'a photo of a beetle',\n",
       " 'a photo of a bison',\n",
       " 'a photo of a boar',\n",
       " 'a photo of a butterfly',\n",
       " 'a photo of a caterpillar',\n",
       " 'a photo of a chimpanzee',\n",
       " 'a photo of a cockroach',\n",
       " 'a photo of a coyote',\n",
       " 'a photo of a crab',\n",
       " 'a photo of a crow',\n",
       " 'a photo of a deer',\n",
       " 'a photo of a dolphin',\n",
       " 'a photo of a donkey',\n",
       " 'a photo of a dragonfly',\n",
       " 'a photo of a eagle',\n",
       " 'a photo of a elephant',\n",
       " 'a photo of a flamingo',\n",
       " 'a photo of a fly',\n",
       " 'a photo of a fox',\n",
       " 'a photo of a goat',\n",
       " 'a photo of a goldfish',\n",
       " 'a photo of a goose',\n",
       " 'a photo of a gorilla',\n",
       " 'a photo of a grasshopper',\n",
       " 'a photo of a hamster',\n",
       " 'a photo of a hare',\n",
       " 'a photo of a hedgehog',\n",
       " 'a photo of a hippopotamus',\n",
       " 'a photo of a hornbill',\n",
       " 'a photo of a hummingbird',\n",
       " 'a photo of a hyena',\n",
       " 'a photo of a jellyfish',\n",
       " 'a photo of a kangaroo',\n",
       " 'a photo of a koala',\n",
       " 'a photo of a ladybugs',\n",
       " 'a photo of a leopard',\n",
       " 'a photo of a lion',\n",
       " 'a photo of a lizard',\n",
       " 'a photo of a lobster',\n",
       " 'a photo of a mosquito',\n",
       " 'a photo of a moth',\n",
       " 'a photo of a mouse',\n",
       " 'a photo of a octopus',\n",
       " 'a photo of a okapi',\n",
       " 'a photo of a orangutan',\n",
       " 'boooooo',\n",
       " 'a photo of a owl',\n",
       " 'a photo of a ox',\n",
       " 'a photo of a oyster',\n",
       " 'a photo of a panda',\n",
       " 'a photo of a parrot',\n",
       " 'a photo of a pelecaniformes',\n",
       " 'a photo of a penguin',\n",
       " 'a photo of a pigeon',\n",
       " 'a photo of a porcupine',\n",
       " 'a photo of a possum',\n",
       " 'a photo of a raccoon',\n",
       " 'a photo of a rat',\n",
       " 'a photo of a reindeer',\n",
       " 'a photo of a rhinoceros',\n",
       " 'a photo of a sandpiper',\n",
       " 'a photo of a seahorse',\n",
       " 'a photo of a seal',\n",
       " 'a photo of a shark',\n",
       " 'a photo of a sheep',\n",
       " 'a photo of a snake',\n",
       " 'a photo of a sparrow',\n",
       " 'a photo of a squid',\n",
       " 'a photo of a squirrel',\n",
       " 'a photo of a starfish',\n",
       " 'a photo of a swan',\n",
       " 'a photo of a tiger',\n",
       " 'a photo of a turkey',\n",
       " 'a photo of a turtle',\n",
       " 'a photo of a whale',\n",
       " 'a photo of a wolf',\n",
       " 'a photo of a wombat',\n",
       " 'a photo of a woodpecker',\n",
       " 'a photo of a zebra']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "clip = AutoModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "for param in clip.parameters():\n",
    "    param.requires_grad = False\n",
    "vision_model = clip.vision_model\n",
    "visual_projection = clip.visual_projection\n",
    "text_projection = clip.text_projection\n",
    "train_prompts = clip.text_model(**train_inputs.to('cuda'))\n",
    "train_prompts = sigmoid(text_projection(train_prompts[1]))\n",
    "test_prompts = clip.text_model(**test_inputs.to('cuda'))\n",
    "test_prompts = sigmoid(text_projection(test_prompts[1]))\n",
    "model = WhereIsFeatures()\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/315 [==============================] - 14s 41ms/step - loss: 0.0508 - std_acc: 0.1125 - recon_acc: 0.0883\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 0.0097 - std_acc: 0.9909 - recon_acc: 0.9434\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                                t_initial=5,\n",
    "                                t_mul=1,\n",
    "                                lr_min=5e-5,\n",
    "                                decay_rate=0.1,\n",
    "                                cycle_limit=1,\n",
    "                                t_in_epochs=False,\n",
    "                                noise_range_t=None,\n",
    "                                )\n",
    "model.train()\n",
    "progbar = tf.keras.utils.Progbar(len(train_loader))\n",
    "for idx, (image, _, cls) in enumerate(train_loader):\n",
    "    image = image.to(device)\n",
    "    cls = cls.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = vision_model(image)['pooler_output']\n",
    "        features = visual_projection(features)\n",
    "        features = sigmoid(features)\n",
    "        std_acc = accuracy(train_prompts, features, cls)\n",
    "\n",
    "    x = model.encode(features)\n",
    "    recon = model.decode(x)\n",
    "    recon_acc = accuracy(train_prompts, recon, cls)\n",
    "    loss = mse(recon, features)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                ]\n",
    "    progbar.update(idx + 1, printlog)\n",
    "model.eval()\n",
    "progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "for idx, (image, _, cls) in enumerate(test_loader):\n",
    "    image = image.to(device)\n",
    "    cls = cls.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = vision_model(image)['pooler_output']\n",
    "        features = visual_projection(features)\n",
    "        features = sigmoid(features)\n",
    "        std_acc = accuracy(test_prompts, features, cls)\n",
    "        x = model.encode(features)\n",
    "        recon = model.decode(x)\n",
    "        recon_acc = accuracy(test_prompts, recon, cls)\n",
    "        loss = mse(recon, features)\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                    ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logits_per_image = torch.matmul(test_prompts, features.t()).t()\n",
    "probs = logits_per_image.softmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: buffer nowhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "315/315 [==============================] - 16s 50ms/step - loss: 0.4965 - std_acc: 0.1143 - recon_acc: 0.0119 - buffer_acc: 0.0179\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.3511 - std_acc: 0.9909 - recon_acc: 0.7541 - buffer_acc: 0.6403\n",
      "Epoch: 2\n",
      "315/315 [==============================] - 17s 52ms/step - loss: 0.4684 - std_acc: 0.1127 - recon_acc: 0.0119 - buffer_acc: 0.0440\n",
      "1250/1250 [==============================] - 48s 38ms/step - loss: 0.3462 - std_acc: 0.9916 - recon_acc: 0.8253 - buffer_acc: 0.8542\n",
      "Epoch: 3\n",
      "315/315 [==============================] - 16s 50ms/step - loss: 0.4638 - std_acc: 0.1111 - recon_acc: 0.0115 - buffer_acc: 0.1268\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.3409 - std_acc: 0.9918 - recon_acc: 0.7983 - buffer_acc: 0.7023\n",
      "Epoch: 4\n",
      "315/315 [==============================] - 17s 52ms/step - loss: 0.4612 - std_acc: 0.1125 - recon_acc: 0.0117 - buffer_acc: 0.2278\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.3375 - std_acc: 0.9915 - recon_acc: 0.8041 - buffer_acc: 0.8747\n",
      "Epoch: 5\n",
      "315/315 [==============================] - 17s 52ms/step - loss: 0.4592 - std_acc: 0.1123 - recon_acc: 0.0101 - buffer_acc: 0.2841\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.3361 - std_acc: 0.9913 - recon_acc: 0.7736 - buffer_acc: 0.9448\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                             warmup_t=1,\n",
    "                             warmup_lr_init=1e-5,\n",
    "                             t_initial=n_epochs,\n",
    "                             t_mul=1,\n",
    "                             lr_min=5e-5,\n",
    "                             decay_rate=0.1,\n",
    "                             cycle_limit=1,\n",
    "                             t_in_epochs=False,\n",
    "                             noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(len(train_loader))\n",
    "    for idx, (image, _, cls) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        cls = cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(train_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(train_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, False)\n",
    "            x = model.encode(features)\n",
    "        x, ecd, gt = model.where(x, False)\n",
    "        buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "        with torch.no_grad():\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(train_prompts, recon, cls)\n",
    "        loss = mse(x, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                    ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                    ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(test_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, False)\n",
    "            x = model.encode(features)\n",
    "            x, ecd, gt = model.where(x, False)\n",
    "            buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(x, gt)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "        progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder: buffer where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "315/315 [==============================] - 16s 48ms/step - loss: 0.4646 - std_acc: 0.1113 - recon_acc: 0.0117 - buffer_acc: 0.2762\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.3399 - std_acc: 0.9916 - recon_acc: 0.6390 - buffer_acc: 0.9645\n",
      "Epoch: 2\n",
      "315/315 [==============================] - 16s 50ms/step - loss: 0.4614 - std_acc: 0.1101 - recon_acc: 0.0125 - buffer_acc: 0.2760\n",
      "1250/1250 [==============================] - 48s 38ms/step - loss: 0.3387 - std_acc: 0.9912 - recon_acc: 0.6697 - buffer_acc: 0.9653\n",
      "Epoch: 3\n",
      "315/315 [==============================] - 16s 50ms/step - loss: 0.4605 - std_acc: 0.1127 - recon_acc: 0.0123 - buffer_acc: 0.2794\n",
      "1250/1250 [==============================] - 48s 38ms/step - loss: 0.3379 - std_acc: 0.9906 - recon_acc: 0.7022 - buffer_acc: 0.9634\n",
      "Epoch: 4\n",
      "315/315 [==============================] - 17s 52ms/step - loss: 0.4600 - std_acc: 0.1111 - recon_acc: 0.0117 - buffer_acc: 0.2760\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.3373 - std_acc: 0.9913 - recon_acc: 0.6940 - buffer_acc: 0.9644\n",
      "Epoch: 5\n",
      "315/315 [==============================] - 17s 52ms/step - loss: 0.4600 - std_acc: 0.1119 - recon_acc: 0.0119 - buffer_acc: 0.2796\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 0.3371 - std_acc: 0.9914 - recon_acc: 0.7139 - buffer_acc: 0.9654\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule = CosineLRScheduler(optimizer,\n",
    "                             warmup_t=1,\n",
    "                             warmup_lr_init=1e-5,\n",
    "                             t_initial=n_epochs,\n",
    "                             t_mul=1,\n",
    "                             lr_min=5e-5,\n",
    "                             decay_rate=0.1,\n",
    "                             cycle_limit=1,\n",
    "                             t_in_epochs=False,\n",
    "                             noise_range_t=None,\n",
    "                                )\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    progbar = tf.keras.utils.Progbar(len(train_loader))\n",
    "    for idx, (image, _, cls) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        cls = cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(train_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(train_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, True)\n",
    "            x = model.encode(features)\n",
    "        x, ecd, gt = model.where(x, True)\n",
    "        buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "        with torch.no_grad():\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(train_prompts, recon, cls)\n",
    "        loss = mse(x, gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                    ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                    ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                    ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                    ]\n",
    "        progbar.update(idx + 1, printlog)\n",
    "    model.eval()\n",
    "    progbar = tf.keras.utils.Progbar(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, _, cls) in enumerate(test_loader):\n",
    "            image = image.to(device)\n",
    "            cls = cls.to(device)\n",
    "            features = vision_model(image)['pooler_output']\n",
    "            features = visual_projection(features)\n",
    "            features = sigmoid(features)\n",
    "            std_acc = accuracy(test_prompts, features, cls)\n",
    "            prompts_ecd = model.encode(test_prompts)\n",
    "            _, prompts_ecd, _ = model.where(prompts_ecd, True)\n",
    "            x = model.encode(features)\n",
    "            x, ecd, gt = model.where(x, True)\n",
    "            buffer_acc = accuracy(prompts_ecd[:, 0], ecd[:, 0], cls)\n",
    "            recon = model.decode(x)\n",
    "            recon_acc = accuracy(test_prompts, recon, cls)\n",
    "            loss = mse(x, gt)\n",
    "            printlog = [('loss', loss.cpu().detach().numpy()),\n",
    "                        ('std_acc', std_acc.cpu().detach().numpy()),\n",
    "                        ('recon_acc', recon_acc.cpu().detach().numpy()),\n",
    "                        ('buffer_acc', buffer_acc.cpu().detach().numpy()),\n",
    "                        ]\n",
    "        progbar.update(idx + 1, printlog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = datetime.now() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=6, microseconds=652361)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2 - n1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47aa4ac3713e7d9a8d444fbbb3116eb5ae369e2c784d7b1a13c1cbac73ef9c1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
